{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/artkulak/idao\" target=\"_blank\">https://app.wandb.ai/artkulak/idao</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/artkulak/idao/runs/3gktldhb\" target=\"_blank\">https://app.wandb.ai/artkulak/idao/runs/3gktldhb</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "wandb: Wandb version 0.10.11 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# WANDB INTEGRATION\n",
    "###################\n",
    "\n",
    "\n",
    "DO_LOGGING = True\n",
    "\n",
    "if DO_LOGGING:\n",
    "    import wandb\n",
    "    wandb.init(project=\"idao\", name=\"Lgbm early stop 8 folds\")\n",
    "\n",
    "\n",
    "###############\n",
    "# IMPORT LIBS\n",
    "###############\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "###############\n",
    "# SET CONSTANTS\n",
    "###############\n",
    "\n",
    "PATH = 'idao2020/'\n",
    "MODELS_PATH = 'models/'\n",
    "COLS_TO_DROP = ['target','card_id']\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(MODELS_PATH)\n",
    "    os.mkdir(MODELS_PATH)\n",
    "except:\n",
    "    os.mkdir(MODELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# SET NOTEBOOK PARAMS\n",
    "#####################\n",
    "\n",
    "eval_strategy = 'kfold' # single\n",
    "MODEL_NAME = 'lgb' # catboost\n",
    "NSEED = 5\n",
    "BASE_SEED = 1000\n",
    "NFOLD = 5\n",
    "\n",
    "VARIANCE_THRESHOLD = 1\n",
    "PCA_COMPONENTS = 30\n",
    "FIXED_SEEDS = [948, 534, 432, 597, 103, 21, 2242, 17, 20, 29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# DEFINE FUNCS\n",
    "###############\n",
    "\n",
    "\n",
    "def roc_auc_score_at_K(predicted_proba, target, rate=0.1): \n",
    "    '''\n",
    "    Competition metric\n",
    "    '''\n",
    "    order = np.argsort(-predicted_proba) \n",
    "    top_k = int(rate * len(predicted_proba)) \n",
    "    return roc_auc_score(target[order][:top_k], predicted_proba[order][:top_k])\n",
    "\n",
    "\n",
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# READ DATA\n",
    "###############\n",
    "\n",
    "data = pd.read_csv(PATH + \"train.csv\")\n",
    "POSTPROC_THRESH = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation(data):\n",
    "    \n",
    "    '''\n",
    "    Generate new features\n",
    "    '''\n",
    "    \n",
    "    df = data.copy()\n",
    "    \n",
    "    def diff_features(df, ft1, ft2):\n",
    "        df['diff-{}-{}'.format(ft1, ft2)] = df[ft1] - df[ft2]\n",
    "        return df\n",
    "\n",
    "    def ratio_features(df, ft2, ft1):\n",
    "        df['ratio-{}-{}'.format(ft1, ft2)] = df[ft1] / (df[ft2] + 1e-12)\n",
    "        return df\n",
    "\n",
    "    def match_features(df, ft1, ft2):\n",
    "        df['match-{}-{}'.format(ft1, ft2)] = (df[ft1] == df[ft2]).astype(int)\n",
    "        return df\n",
    "\n",
    "    def common_regions(df, ft1, ft2):\n",
    "        missed_regions = []\n",
    "        used_regions = []\n",
    "        for reg, count in df[ft1].value_counts().items():\n",
    "            if count in df[ft2].value_counts().values:\n",
    "                used_regions.append(reg)\n",
    "            else:\n",
    "                missed_regions.append(reg)\n",
    "        df['common_regions-{}-{}'.format(ft1, ft2)] = df[ft1].apply(lambda x: int(x in used_regions))\n",
    "        return df\n",
    "    \n",
    "    def comb_features(df, ft1, ft2):\n",
    "        df['comb-{}-{}'.format(ft1, ft2)] = df[ft1].astype(str) + '-' + df[ft2].astype(str)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    ##### v01 #####\n",
    "\n",
    "#     for col in [\n",
    "#         'addr_region_fact_encoding1',\n",
    "#         'addr_region_reg_encoding1',\n",
    "#         'app_addr_region_reg_encoding1',\n",
    "#         'app_addr_region_fact_encoding1',\n",
    "#     ]:\n",
    "#         df[col] = np.round(df[col] * 0.0083, 6).astype(int)\n",
    "#     df['app_addr_region_sale_encoding1'] = np.round(df['app_addr_region_sale_encoding1'] * 0.0039).astype(int)\n",
    "#     for col in [\n",
    "#         'addr_region_fact_encoding2',\n",
    "#         'addr_region_reg_encoding2',\n",
    "#         'app_addr_region_reg_encoding2',\n",
    "#         'app_addr_region_fact_encoding2',\n",
    "#     ]:\n",
    "#         df[col] = np.round(df[col] * 1.1).astype(int)\n",
    "#     df['app_addr_region_sale_encoding2'] = np.round(df['app_addr_region_sale_encoding2'] * 0.007).astype(int)\n",
    "\n",
    "#     df = diff_features(df, 'sas_limit_after_003_amt', 'sas_limit_last_amt')\n",
    "\n",
    "#     df = match_features(df, 'sas_limit_after_003_amt', 'sas_limit_last_amt')\n",
    "\n",
    "#     df = match_features(df, 'addr_region_fact_encoding1', 'addr_region_reg_encoding1')\n",
    "#     df = match_features(df, 'addr_region_fact_encoding1', 'app_addr_region_reg_encoding1')\n",
    "#     df = match_features(df, 'addr_region_fact_encoding1', 'app_addr_region_fact_encoding1')\n",
    "#     df = match_features(df, 'addr_region_reg_encoding1', 'app_addr_region_reg_encoding1')\n",
    "#     df = match_features(df, 'addr_region_reg_encoding1', 'app_addr_region_fact_encoding1')\n",
    "#     df = match_features(df, 'app_addr_region_reg_encoding1', 'app_addr_region_fact_encoding1')\n",
    "\n",
    "#     df = match_features(df, 'addr_region_fact_encoding2', 'addr_region_reg_encoding2')\n",
    "#     df = match_features(df, 'addr_region_fact_encoding2', 'app_addr_region_reg_encoding2')\n",
    "#     df = match_features(df, 'addr_region_fact_encoding2', 'app_addr_region_fact_encoding2')\n",
    "#     df = match_features(df, 'addr_region_reg_encoding2', 'app_addr_region_reg_encoding2')\n",
    "#     df = match_features(df, 'addr_region_reg_encoding2', 'app_addr_region_fact_encoding2')\n",
    "#     df = match_features(df, 'app_addr_region_reg_encoding2', 'app_addr_region_fact_encoding2')\n",
    "\n",
    "#     df = common_regions(df, 'addr_region_fact', 'addr_region_fact_encoding1')\n",
    "#     df = common_regions(df, 'addr_region_reg', 'addr_region_reg_encoding1')\n",
    "#     df = common_regions(df, 'app_addr_region_reg', 'app_addr_region_reg_encoding1')\n",
    "#     df = common_regions(df, 'app_addr_region_fact', 'app_addr_region_fact_encoding1')\n",
    "#     df = common_regions(df, 'app_addr_region_sale', 'app_addr_region_sale_encoding1')\n",
    "\n",
    "    ##### v02 #####\n",
    "\n",
    "#     df = diff_features(df, 'first_loan_date', 'last_loan_date')\n",
    "#     df = ratio_features(df, 'first_loan_date', 'last_loan_date')\n",
    "#     df = match_features(df, 'first_loan_date', 'last_loan_date')\n",
    "\n",
    "#     df = diff_features(df, 'clnt_experience_cur_mnth', 'clnt_experience_total_mnth')\n",
    "\n",
    "#     df = diff_features(df, 'ttl_inquiries', 'inquiry_1_week')\n",
    "#     df = diff_features(df, 'inquiry_12_month', 'inquiry_1_week')\n",
    "#     df = diff_features(df, 'inquiry_9_month', 'inquiry_1_week')\n",
    "#     df = diff_features(df, 'inquiry_6_month', 'inquiry_1_week')\n",
    "#     df = diff_features(df, 'inquiry_3_month', 'inquiry_1_week')\n",
    "#     df = diff_features(df, 'ttl_inquiries', 'inquiry_3_month')\n",
    "#     df = diff_features(df, 'inquiry_12_month', 'inquiry_3_month')\n",
    "#     df = diff_features(df, 'inquiry_9_month', 'inquiry_3_month')\n",
    "#     df = diff_features(df, 'inquiry_6_month', 'inquiry_3_month')\n",
    "#     df = diff_features(df, 'ttl_inquiries', 'inquiry_6_month')\n",
    "#     df = diff_features(df, 'inquiry_12_month', 'inquiry_6_month')\n",
    "#     df = diff_features(df, 'inquiry_9_month', 'inquiry_6_month')\n",
    "#     df = diff_features(df, 'ttl_inquiries', 'inquiry_9_month')\n",
    "#     df = diff_features(df, 'inquiry_12_month', 'inquiry_9_month')\n",
    "#     df = diff_features(df, 'ttl_inquiries', 'inquiry_12_month')\n",
    "\n",
    "#     df = ratio_features(df, 'ttl_inquiries', 'inquiry_1_week')\n",
    "#     df = ratio_features(df, 'inquiry_12_month', 'inquiry_1_week')\n",
    "#     df = ratio_features(df, 'inquiry_9_month', 'inquiry_1_week')\n",
    "#     df = ratio_features(df, 'inquiry_6_month', 'inquiry_1_week')\n",
    "#     df = ratio_features(df, 'inquiry_3_month', 'inquiry_1_week')\n",
    "#     df = ratio_features(df, 'ttl_inquiries', 'inquiry_3_month')\n",
    "#     df = ratio_features(df, 'inquiry_12_month', 'inquiry_3_month')\n",
    "#     df = ratio_features(df, 'inquiry_9_month', 'inquiry_3_month')\n",
    "#     df = ratio_features(df, 'inquiry_6_month', 'inquiry_3_month')\n",
    "#     df = ratio_features(df, 'ttl_inquiries', 'inquiry_6_month')\n",
    "#     df = ratio_features(df, 'inquiry_12_month', 'inquiry_6_month')\n",
    "#     df = ratio_features(df, 'inquiry_9_month', 'inquiry_6_month')\n",
    "#     df = ratio_features(df, 'ttl_inquiries', 'inquiry_9_month')\n",
    "#     df = ratio_features(df, 'inquiry_12_month', 'inquiry_9_month')\n",
    "#     df = ratio_features(df, 'ttl_inquiries', 'inquiry_12_month')\n",
    "    \n",
    "#     ##### v03 #####\n",
    "\n",
    "    df = comb_features(df, 'prt_name', 'channel_name_2')\n",
    "    df = comb_features(df, 'prt_name', 'clnt_income_month_avg_net_amt') # 132 categories\n",
    "    df = comb_features(df, 'prt_name', 'clnt_birth_year') # 503 categories\n",
    "    df = comb_features(df, 'prt_name', 'inquiry_1_week') # 222 categories\n",
    "    df = comb_features(df, 'prt_name', 'addr_region_fact') # 464 categories\n",
    "    df = comb_features(df, 'prt_name', 'sas_limit_last_amt') # 142 categories\n",
    "    df = comb_features(df, 'prt_name', 'clnt_speciality_sphere_name') # 250 categories\n",
    "    df = comb_features(df, 'prt_name', 'addr_region_fact_encoding1') # 234 categories\n",
    "    \n",
    "    \n",
    "    ########## v04 #############\n",
    "#     # df['fe_col01'] = df.apply(lambda x: 1 if\n",
    "#     #          (x['inquiry_1_week'] == x['inquiry_1_month']) & \\\n",
    "#     #          (x['inquiry_1_month'] == x['inquiry_3_month']) & \\\n",
    "#     #          (x['inquiry_3_month'] == x['inquiry_6_month']) & \\\n",
    "#     #          (x['inquiry_6_month'] == x['inquiry_9_month']) & \\\n",
    "#     #          (x['inquiry_9_month'] == x['inquiry_12_month'])\n",
    "#     #         else 0, axis=1)\n",
    "#     df['fe_feature_nans'] = sum([df[col].isna() for col in ['feature_10'] + ['feature_{}'.format(x) for x in range(12, 30)]])\n",
    "#     df['fe_inquiry_mean_diff'] = df['inquiry_1_week'] / 7 + \\\n",
    "#             (df['inquiry_1_month'] - df['inquiry_1_week']) / 23 + \\\n",
    "#             (df['inquiry_3_month'] - df['inquiry_1_month']) / 60 + \\\n",
    "#             (df['inquiry_6_month'] - df['inquiry_3_month']) / 90 + \\\n",
    "#             (df['inquiry_9_month'] - df['inquiry_6_month']) / 90 + \\\n",
    "#             (df['inquiry_12_month'] - df['inquiry_9_month']) / 90\n",
    "#     df['fe_inquiry_anomaly_day'] = df.apply(lambda x: 1 if\n",
    "#                                             ((x['inquiry_recent_period'] <= 30) & (x['inquiry_1_month'] == 0)) | \\\n",
    "#                                             ((x['inquiry_recent_period'] <= 89) & (x['inquiry_3_month'] == 0)) | \\\n",
    "#                                             ((x['inquiry_recent_period'] <= 181) & (x['inquiry_6_month'] == 0)) | \\\n",
    "#                                             ((x['inquiry_recent_period'] <= 273) & (x['inquiry_9_month'] == 0)) | \\\n",
    "#                                             ((x['inquiry_recent_period'] <= 273) & (x['inquiry_12_month'] == 0)) | \\\n",
    "#                                             ((x['inquiry_recent_period'] <= 365) & (x['inquiry_12_month'] == 0))\n",
    "#                                            else 0, axis=1)\n",
    "#     df['fe_inquiry_mean_before_12_month'] = (df['ttl_inquiries'] - df['inquiry_12_month']) / (df['first_loan_date'] + 366)\n",
    "\n",
    "#     df = diff_features(df, 'loans_main_borrower', 'loans_active')\n",
    "#     df = match_features(df, 'addr_region_reg', 'addr_region_fact')\n",
    "\n",
    "#     df = diff_features(df, 'inquiry_1_month', 'inquiry_1_week')\n",
    "#     df = diff_features(df, 'inquiry_3_month', 'inquiry_1_month')\n",
    "#     df = diff_features(df, 'inquiry_6_month', 'inquiry_3_month')\n",
    "#     df = diff_features(df, 'inquiry_9_month', 'inquiry_6_month')\n",
    "#     df = diff_features(df, 'inquiry_12_month', 'inquiry_9_month')\n",
    "#     df = diff_features(df, 'ttl_inquiries', 'inquiry_12_month')\n",
    "\n",
    "#     df['inquiry_1_week*sas_limit_last_amt'] = df['inquiry_1_week']*df['sas_limit_last_amt']\n",
    "#     df['inquiry_1_month*sas_limit_last_amt'] = df['inquiry_1_month']*df['sas_limit_last_amt']\n",
    "#     df['inquiry_recent_period*sas_limit_last_amt'] = df['inquiry_recent_period']*df['sas_limit_last_amt']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# PREPROCESS DATA\n",
    "#################\n",
    "\n",
    "\n",
    "def add_pca(data):\n",
    "    '''\n",
    "    Add pca features to data\n",
    "    '''\n",
    "    \n",
    "    pca = PCA(n_components=PCA_COMPONENTS, random_state=BASE_SEED)\n",
    "    pcas = pca.fit_transform(data.iloc[:, 2:])\n",
    "    pcas = pd.DataFrame(pcas, columns=[f'pca_{i}' for i in range(PCA_COMPONENTS)])\n",
    "    data_all = pd.concat([data, pcas], axis=1)\n",
    "    \n",
    "    \n",
    "    # save pca for inference\n",
    "    joblib.dump(pca, MODELS_PATH + f'pca.pkl')\n",
    "    \n",
    "    return data_all\n",
    "\n",
    "def variance_threshold(data):\n",
    "    '''\n",
    "    Filter data based on variance threshold\n",
    "    '''\n",
    "    cols_numeric = []\n",
    "    for col in data.columns[2:]:\n",
    "        if data[col].dtype != 'object':\n",
    "                cols_numeric.append(col)\n",
    "                \n",
    "    mask = (data[cols_numeric].var() < VARIANCE_THRESHOLD).values\n",
    "    data.drop(columns = np.array(cols_numeric)[mask], inplace = True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def preprocess_data(data):\n",
    "    '''\n",
    "    Fill nans and label encode\n",
    "    '''\n",
    "    for i, col in enumerate(data.columns[2:]):\n",
    "        if data[col].dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            data[col] = le.fit_transform(data[col])\n",
    "            joblib.dump(le, MODELS_PATH + f'le_{col}.pkl')\n",
    "            \n",
    "            \n",
    "    return data\n",
    "\n",
    "data = feature_generation(data)\n",
    "data = preprocess_data(data)\n",
    "# data = variance_threshold(data)\n",
    "# data = add_pca(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_train(X_train, Y_train, X_val, Y_val, i_fold=None, seed=None, params = None):\n",
    "    # prepare for train\n",
    "    \n",
    "    params = {\n",
    "              \"objective\": \"cross_entropy_lambda\",\n",
    "                \"boosting\": \"gbdt\",\n",
    "                \"n_estimators\": 3000,\n",
    "                \"num_leaves\": 60,\n",
    "                \"num_threads\": 8,\n",
    "              \"random_state\": seed,\n",
    "              }\n",
    "    \n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params) # define model here\n",
    "    \n",
    "    # Fit and save model\n",
    "    model.fit(X_train, Y_train,   eval_set=(X_val, Y_val), early_stopping_rounds=500, verbose=False)\n",
    "    joblib.dump(model, MODELS_PATH + f'{MODEL_NAME}_{i_fold}_{seed}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold\n",
    "def train_kfold_model(train_df):\n",
    "    X_train_val = train_df.drop(columns = COLS_TO_DROP)\n",
    "    Y_train_val = train_df['target']\n",
    "\n",
    "    oof = np.zeros((X_train_val.shape[0], NSEED)) # cv_score\n",
    "    seeds = []\n",
    "    for i_seed in range(NSEED):\n",
    "        #seed = random.randint(0, BASE_SEED)\n",
    "        seed = FIXED_SEEDS[i_seed]\n",
    "        seed_everything(seed)\n",
    "        \n",
    "        seeds.append(seed)\n",
    "        print('Seed: {}, {}/{}'.format(seed, i_seed + 1, NSEED))\n",
    "        kf = KFold(n_splits=NFOLD, random_state=seed, shuffle=True)\n",
    "        for i_fold, (train_idx, val_idx) in enumerate(kf.split(X_train_val, Y_train_val)):\n",
    "            print(\"# Fold: {}/{} (seed: {}/{})\".format(i_fold + 1, NFOLD, i_seed + 1, NSEED))\n",
    "\n",
    "            # dataset\n",
    "            X_train, Y_train = X_train_val.iloc[train_idx], Y_train_val[train_idx]\n",
    "            X_val, Y_val = X_train_val.iloc[val_idx], Y_train_val[val_idx]\n",
    "                \n",
    "\n",
    "            # train\n",
    "            running_train(X_train, Y_train, X_val, Y_val, i_fold=i_fold, seed=seed)\n",
    "\n",
    "            # predict on oof\n",
    "            print('predict on oof...', end='')\n",
    "            model = joblib.load(MODELS_PATH + f'{MODEL_NAME}_{i_fold}_{seed}.pkl')\n",
    "            \n",
    "            prediction = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "            oof[val_idx, i_seed] = prediction\n",
    "            print('  done.')\n",
    "    \n",
    "    oof = np.mean(oof, axis = 1)\n",
    "    cv_rocauc = roc_auc_score(Y_train_val, oof)\n",
    "    oof = postprocess_predictions(oof)\n",
    "    cv_score = roc_auc_score_at_K(oof, Y_train_val)\n",
    "    \n",
    "    print('{} folds ROC AUC top 10: {:.5f} ROC AUC {:.5f}'.format(NFOLD, cv_score, cv_rocauc))\n",
    "    if DO_LOGGING:\n",
    "        wandb.summary['rocauc_10']= cv_score\n",
    "        wandb.summary['rocauc_all']= cv_rocauc\n",
    "    \n",
    "    return seeds, oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single model\n",
    "def train_single_model(train_df):\n",
    "    # obtain train, val, test\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.2, shuffle=True, random_state=BASE_SEED)\n",
    "\n",
    "    X_train, X_val, Y_train, Y_val = train_df.drop(columns = COLS_TO_DROP), val_df.drop(columns = COLS_TO_DROP),\\\n",
    "    train_df['target'], val_df['target']\n",
    "    \n",
    "    \n",
    "    # train\n",
    "    running_train(X_train, Y_train, X_val, Y_val, 0, 0)\n",
    "\n",
    "    # predict on test\n",
    "    print('predict on valid...', end='')\n",
    "    model = joblib.load(MODELS_PATH + f'{MODEL_NAME}_{0}_{0}.pkl')\n",
    "    predictions = model.predict_proba(X_val)[:, 1]\n",
    "    print('  done.\\n')\n",
    "    \n",
    "    cv_rocauc = roc_auc_score(Y_val, predictions)\n",
    "    predictions = postprocess_predictions(predictions)\n",
    "    cv_score = roc_auc_score_at_K(predictions, Y_val.values)\n",
    "    \n",
    "    print('ROC AUC top 10: {:.5f} ROC AUC {:.5f}'.format(cv_score, cv_rocauc))\n",
    "    if DO_LOGGING:\n",
    "        wandb.summary['rocauc_10']= cv_score\n",
    "        wandb.summary['rocauc_all']= cv_rocauc\n",
    "    \n",
    "    return predictions, Y_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_predictions(predictions, rate = 0.1):\n",
    "    thresh = POSTPROC_THRESH\n",
    "    order = np.argsort(-predictions)  # -> 5%: лучшие (единицы) | 90% занулили (все что между) | 5% оставили (нули)\n",
    "    top_k = int(0.1 * len(predictions))\n",
    "    \n",
    "    predictions[order[int(thresh * len(predictions)):int((1 - rate + thresh) * len(predictions))]] = -1\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 948, 1/5\n",
      "# Fold: 1/5 (seed: 1/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 2/5 (seed: 1/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 3/5 (seed: 1/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 4/5 (seed: 1/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 5/5 (seed: 1/5)\n",
      "predict on oof...  done.\n",
      "Seed: 534, 2/5\n",
      "# Fold: 1/5 (seed: 2/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 2/5 (seed: 2/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 3/5 (seed: 2/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 4/5 (seed: 2/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 5/5 (seed: 2/5)\n",
      "predict on oof...  done.\n",
      "Seed: 432, 3/5\n",
      "# Fold: 1/5 (seed: 3/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 2/5 (seed: 3/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 3/5 (seed: 3/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 4/5 (seed: 3/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 5/5 (seed: 3/5)\n",
      "predict on oof...  done.\n",
      "Seed: 597, 4/5\n",
      "# Fold: 1/5 (seed: 4/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 2/5 (seed: 4/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 3/5 (seed: 4/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 4/5 (seed: 4/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 5/5 (seed: 4/5)\n",
      "predict on oof...  done.\n",
      "Seed: 103, 5/5\n",
      "# Fold: 1/5 (seed: 5/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 2/5 (seed: 5/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 3/5 (seed: 5/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 4/5 (seed: 5/5)\n",
      "predict on oof...  done.\n",
      "# Fold: 5/5 (seed: 5/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict on oof...  done.\n",
      "5 folds ROC AUC top 10: 0.91032 ROC AUC 0.74480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.10.11 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "if eval_strategy == 'kfold':\n",
    "    seeds, predictions = train_kfold_model(data)\n",
    "elif eval_strategy == 'single':\n",
    "    predictions, y_val = train_single_model(data)\n",
    "else:\n",
    "    print('eval_strategy should be \\\"kfold\\\" or \\\"single\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xaf22e03a20>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEQRJREFUeJzt3X+M5HV9x/Hne3/db7lbbjmQA+4gFLVaI2xBxCoBTQk1YlNtsFHR0l6wscWmScWY1qQ/UrWNqY1ae1UqWATi+QstVlCg1ARO9/h5cMqvChyc3CJwctxx++vdP2bAZdkfs/Odndn7+Hwkk5n5zmfm+9rP7r72O9/5zmxkJpKkg19XpwNIklrDQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVoqedK1u7dm1u2LChnauUpIPetm3bHs/MgbnGtbXQN2zYwNDQUDtXKUkHvYh4sJFx7nKRpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhZiz0CPi4ojYHRHbJy37x4j4cUTcERFfj4jVCxtTkjSXRrbQvwicNWXZtcArM/M3gHuAD7c4lyRpnuYs9My8EXhiyrJrMnOsfvVmYP0CZJMkzUMr3in6h8CVM90YEZuATQBHH3100yv58taHpl3+B6c0/5iSVJJKL4pGxEeAMeCymcZk5ubMHMzMwYGBOT+KQJLUpKa30CPiPOAtwJmZma2LJElqRlOFHhFnAR8C3piZ+1obSZLUjEYOW7wcuAk4ISJ2RsT5wKeBVcC1EXFbRHxugXNKkuYw5xZ6Zr5zmsVfWIAskqQKfKeoJBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBVizkKPiIsjYndEbJ+0rD8iro2Ie+vnaxY2piRpLo1soX8ROGvKsouA72fm8cD369clSR00Z6Fn5o3AE1MWnwNcUr98CfC2FueSJM1Ts/vQ12XmLoD6+WGtiyRJasaCvygaEZsiYigihoaHhxd6dZL0K6vZQn8sIo4AqJ/vnmlgZm7OzMHMHBwYGGhydZKkuTRb6FcB59Uvnwd8szVxJEnNauSwxcuBm4ATImJnRJwPfAx4c0TcC7y5fl2S1EE9cw3IzHfOcNOZLc4iSarAd4pKUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEqFXpE/HlE3BUR2yPi8ohY2qpgkqT5abrQI+JI4M+Awcx8JdANnNuqYJKk+am6y6UHWBYRPcBy4NHqkSRJzWi60DPzEeCfgIeAXcCezLymVcEkSfNTZZfLGuAcYCPwUmBFRLxrmnGbImIoIoaGh4ebTypJmlWVXS5vAv4vM4czcxT4GvC6qYMyc3NmDmbm4MDAQIXVSZJmU6XQHwJeGxHLIyKAM4EdrYklSZqvKvvQtwJbgFuAO+uPtblFuSRJ89RT5c6Z+VHgoy3KIkmqwHeKSlIhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKkSlQo+I1RGxJSJ+HBE7IuLUVgWTJM1PT8X7fwr478x8e0T0ActbkEmS1ISmCz0iXgK8AXgvQGaOACOtiSVJmq8qu1yOBYaB/4iIWyPi8xGxokW5JEnzVKXQe4ATgX/NzNcAzwAXTR0UEZsiYigihoaHhyusTpI0myqFvhPYmZlb69e3UCv4F8jMzZk5mJmDAwMDFVYnSZpN04WemT8DHo6IE+qLzgTubkkqSdK8VT3K5U+By+pHuDwAvK96JElSMyoVembeBgy2KIskqQLfKSpJhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5Jhahc6BHRHRG3RsS3WxFIktScVmyhXwjsaMHjSJIqqFToEbEe+B3g862JI0lqVtUt9H8G/hKYaEEWSVIFTRd6RLwF2J2Z2+YYtykihiJiaHh4uNnVSZLmUGUL/TTgrRHxU+AK4IyI+M+pgzJzc2YOZubgwMBAhdVJkmbTdKFn5oczc31mbgDOBa7LzHe1LJkkaV48Dl2SCtHTigfJzBuAG1rxWJKk5riFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRBNF3pEHBUR10fEjoi4KyIubGUwSdL89FS47xjwF5l5S0SsArZFxLWZeXeLskmS5qHpLfTM3JWZt9QvPw3sAI5sVTBJ0vy0ZB96RGwAXgNsnea2TRExFBFDw8PDrVidJGkalQs9IlYCXwU+mJm/mHp7Zm7OzMHMHBwYGKi6OknSDCoVekT0UivzyzLza62JJElqRpWjXAL4ArAjMz/ZukiSpGZU2UI/DXg3cEZE3FY/nd2iXJKkeWr6sMXM/AEQLcwiSarAd4pKUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrqmtWffKJ+5/j6+dfuj7BsZ63QcSQ3o6XQAdd7o+ATv3Hwzvd1dXHD6cSzr7eaDV9zKo3ueBWBpbxcXnvlrvP/04zqcVNJsLHRxxY8eZujBJ+lf0cd5F/8QgP4Vfbz/jccxOjHBdTt288lrf8LSni7e9/qNHU4raSYW+q+4vQfG+NT37uGUjf186fxT+MZtj3DNXT/jtOPWsqS3G4DuVwT/duMD3L3rFx1OK2k2FvqvsC9vfYjv7XiMx/eO8I6T1rBl204AznjZuheMO6p/OauX93L7zqc6EVNSgyoVekScBXwK6AY+n5kfa0kqtcWe/aP84N7HeeWRh3BU//IZx3VF8Or1q/nfe4f5+d4DHLpySRtTqlP27B/lr76xnXsee5qTN/Zz0jFrWFZ/1tbX08Wy3m6W9XWzvK+bZX09rFu1hJ5uj7PopKYLPSK6gc8AbwZ2Aj+KiKsy8+5WhdPC2bN/lEtv+ikAv/2KdbOOBXj1+tX8zz3DXH3nLt596oYFzabO2/7IHt5/2TZ+tudZTjpmDV8Z2smlNz046316u4Oj+5ezckkPj+8dYf/oOMcftpJXHXkIS3u7eWr/CBMJGw5dzoZDV7Ckt5uJTJZ0d7FmRR/9K/pYvbyXJT3dbfoqy1NlC/1k4L7MfAAgIq4AzgEs9EXu2dFx/vjSIXb/4gDvOfWYhra4Dz9kKetesoSv3/oILz/iJdzy0JOsXtbHb27s55j+5Ty5b4Qnnhlh1dJe1q7so7srODA2wYGxCZb3ddNb33KbmEjGM+npCiJiob9UzdN9u/fyoS13cOvDT7JqaS/nv/5Yju5fztmvOoLHnx5hIpMExieSkbEJRsdrpwOjEzyxb4Thpw+wf3Scw1YtobsreOSp/Wx78EnGJ5JlfbWi3jcyPmuGFX3dzxf8qqU99HR10dsd9HR10dMdHLKsl5euXsbAyiWMjE/w7Og4/Sv62LB2Bf3L+3hq/yh79o/S2x2/fBbR28PS3i4igq6oPeuM4EXXn1/Oi8cdDKoU+pHAw5Ou7wROqRZnen/77bv50s3Tbx38/X/592O+xiaSkfEJfv+kozh+3aqG7/fq9au55u7HePvnbnrB8q6Aifzl9QjojmBs0sK+ni5IGBmfeP4+S3q66Zrye5JAJiRZP68trNXIpHUQz1147hIRv7xtvr9/mXOPaYeF7I1GHvqZkXF6u4OTNx7KGS87jJVLahXR09XF4YcsbWq9E/XJ7ap/cftGxnjimREmJpKIYHR8gmdGxtk3Msa+kXH2HRh7/vpT+0aZyGR8onaayKyNmeOPwkKY6Q/BdKb7edr8npP4reMHFjRjlUKf7it50ZcREZuATfWreyPiJ02uby3weJP3bZeDKuMnOhxkBgfVHC5ilTLeB1zZuizTKX4Op3rD31W6+zGNDKpS6DuBoyZdXw88OnVQZm4GNldYDwARMZSZg1UfZyGZsbrFng/M2AqLPR8cHBmnqvKS9I+A4yNiY0T0AecCV7UmliRpvpreQs/MsYj4APBdaoctXpyZd7UsmSRpXiodh56ZVwNXtyjLXCrvtmkDM1a32POBGVthseeDgyPjC0Qulpf3JUmV+LYuSSrEoi30iHhHRNwVERMRMeMrzRFxVkT8JCLui4iL2pyxPyKujYh76+drZhg3HhG31U8L/sLxXHMSEUsi4sr67VsjYsNCZ2oi43sjYnjSvP1Rm/NdHBG7I2L7DLdHRPxLPf8dEXFiO/M1mPH0iNgzaQ7/us35joqI6yNiR/13+cJpxnR0HhvM2NF5nJfMXJQn4OXACcANwOAMY7qB+4FjgT7gduAVbcz4CeCi+uWLgI/PMG5vGzPNOSfAnwCfq18+F7iyzd/bRjK+F/h0B3/+3gCcCGyf4fazge9Qez/Ga4GtizDj6cC3OziHRwAn1i+vAu6Z5vvc0XlsMGNH53E+p0W7hZ6ZOzJzrjchPf/xA5k5Ajz38QPtcg5wSf3yJcDb2rjumTQyJ5NzbwHOjPa+t7nT37c5ZeaNwBOzDDkHuDRrbgZWR8QR7UlX00DGjsrMXZl5S/3y08AOau8wn6yj89hgxoPGoi30Bk338QPt/Gasy8xdUPvBAA6bYdzSiBiKiJsjYqFLv5E5eX5MZo4Be4BDFzjXtOuvm+n79nv1p+FbIuKoaW7vpE7/7DXq1Ii4PSK+ExG/3qkQ9d16rwG2Trlp0czjLBlhkczjXDr6eegR8T3g8Glu+khmfrORh5hmWUsP25kt4zwe5ujMfDQijgWui4g7M/P+1iR8kUbmZMHnbQ6NrP9bwOWZeSAiLqD2jOKMBU/WuE7PYSNuAY7JzL0RcTbwDeD4doeIiJXAV4EPZubU/5KyKOZxjoyLYh4b0dFCz8w3VXyIhj5+oIrZMkbEYxFxRGbuqj9N3D3DYzxaP38gIm6gthWwUIXeyJw8N2ZnRPQAh9Dep+5zZszMn0+6+u/Ax9uQaz4W/GevqsnFlJlXR8RnI2JtZrbtM1QiopdaUV6WmV+bZkjH53GujIthHht1sO9y6fTHD1wFnFe/fB7womcVEbEmIpbUL68FTmNhP2K4kTmZnPvtwHVZf/WnTebMOGU/6lup7dtcTK4C3lM/SuO1wJ7ndr8tFhFx+HOvjUTEydR+338++71auv4AvgDsyMxPzjCso/PYSMZOz+O8dPpV2ZlOwO9S++t9AHgM+G59+UuBqyeNO5vaK9P3U9tV086MhwLfB+6tn/fXlw9S+w9OAK8D7qR2JMedwPltyPWiOQH+Bnhr/fJS4CvUPlTvh8CxHfj+zpXxH4C76vN2PfCyNue7HNgFjNZ/Ds8HLgAuqN8e1P7By/317+u0R2J1OOMHJs3hzcDr2pzv9dR2n9wB3FY/nb2Y5rHBjB2dx/mcfKeoJBXiYN/lIkmqs9AlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSrE/wN5Qo1dGOmulAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "wandb: Wandb version 0.10.11 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "wandb.summary['rocauc_10']= 0.91248\n",
    "wandb.summary['rocauc_all']= 0.74580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "wandb: Wandb version 0.10.11 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "if DO_LOGGING:\n",
    "    wandb.summary['lb_score'] =89.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# FEAT IMPORTANCE\n",
    "#################\n",
    "\n",
    "\n",
    "if eval_strategy == 'single':\n",
    "    model = joblib.load(MODELS_PATH + f'{MODEL_NAME}_{0}_{0}.pkl')\n",
    "    lgb.plot_importance(model, max_num_features = 15)\n",
    "\n",
    "    importances = pd.DataFrame(sorted(zip(model.feature_importances_,data.columns[2:])), columns=['Value','Feature'])\n",
    "    importances.to_csv('importances.csv', index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
